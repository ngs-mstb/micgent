include required(classpath("application"))

database {
  profile = "slick.jdbc.HsqldbProfile$"
  db {
    driver = "org.hsqldb.jdbcDriver"
    # Use a file in the current directory for in-process DB - this way,
    # we can enable caching and effectively resume the workflow in the same
    # directory if it gets killed (it will actually create new workflow subdirectory but
    # re-use cached data). Be careful to not run two concurrent Cromwell instances
    # in the same directory (lock file should prevent corruption though).
    # Intentionally killing and restarting Cromwell instance
    # is also a work-around to recover from forever-missing jobs (as killed by the
    # LRM and would never having created the expected rc file).
    # WARNING: when resuming large workflows from cache, Cromwell will open a lot of file
    # descriptors for this in-process DB operation and apparent check sum computation, and might
    # exhaust OS ulimit constraints. The following cryptic signs will appear:
    # - log activity stops
    # - cromwell process stays around w/o any CPU use
    # - earlier log entries contain Java tracebacks from some threads with "connection closed" message
    # - resuming from cache takes a very long time (even before all activity stops)
    #
    # Apparently, some Cromwell threads die with these tracebacks, and the workflow gets deadlocked waiting for
    # tasks that were sent to those threads.
    # In such case, either encrease the ulimit on file descriptors, or switch to MySQL DB.
    # Other possible option is to reduce the number of DB-accessing and/or other threads, but I did not
    # see the parameter name in the docs, although log entries hint about them.
    url = "jdbc:hsqldb:file:cromwell.db;shutdown=false;hsqldb.tx=mvcc"

    # 3000 (=3 sec) is not enough on some NFS file systems to start Cromwell - it exits with a time-out
    connectionTimeout = 30000
  }
}

system.io {
  number-of-attempts = 5
}

backend {
  default = "PBS"
  providers {

    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 6
      }
    }

    PBS {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {

        # Limits the number of concurrent jobs
        concurrent-job-limit = 500

        runtime-attributes = """
        Int cpu = 1
        Int? memory_gb
        String? pbs_queue
        String? pbs_account
        # Conda environment can be specified for each job
        String? conda_env
        """

        # This submit command re-writes the job script to activate the requested Conda environment.
        # To avoid hard-coding the path to the Conda installation, it expects that the submitting Cromwell process
        # is started from some Conda environment, the submitting host variables are passed into the job with `qsub -V` option,
        # and then the job script looks for the Conda executable in the current environment.
        # The script expects the latest Conda versions (>= 4.0) that use sourcing of profile.d/conda.sh instead of simply
        # adding Conda bin dir to the PATH.
        submit = """
        touch ${out} ${err}
        if [ -n "${conda_env}" ]; then

        ## Cromwell must be running from some Conda environment to detect
        ## what Conda install to use. Conda env var is dereferenced on the
        ## server side and written into the job script
        conda_exe=$(which conda)
        [ -n "$conda_exe" ] || exit 1
        conda_root=$(dirname $(dirname $conda_exe))
        conda_prof="$conda_root/etc/profile.d/conda.sh"

        script_new=${script+".new"}
        (
        cat <<EOF
        #!/bin/bash
        . $conda_prof
        ${"conda activate " + conda_env}
        EOF
        ) > "$script_new"
            echo "hostname; date; printenv" >> "$script_new"
            cat ${script} >> "$script_new"
            mv "$script_new" ${script}
        fi
        qsub \
        -S /bin/sh \
        -V \
        -N ${job_name} \
        -d ${cwd} \
        -k n \
        -m n \
        -o ${out} \
        -e ${err} \
        ${"-l nodes=1:ppn=" + cpu} \
        ${"-l mem=" + memory_gb + "gb"} \
        ${"-q " + pbs_queue} \
        ${"-A " + pbs_account} \
        ${script}
        """

        kill = "qdel ${job_id}"
        check-alive = "qstat ${job_id}"
        job-id-regex = "(.*)"
      }
    }
  }
}

system {
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true

  # Maximum number of input file bytes allowed in order to read each type.
  # If exceeded a FileSizeTooBig exception will be thrown.
  input-read-limits {

    lines = 1280000

    #bool = 7

    #int = 19

    #float = 50

    string = 1280000

    json = 1280000

    tsv = 1280000

    map = 1280000

    object = 1280000
  }

}

call-caching {
  enabled = true
  invalidate-bad-cache-results = true
}

